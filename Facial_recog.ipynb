{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PxeiJ0XjKDxb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhi4-j6YbDCh",
        "outputId": "0db3d939-caba-413e-f8be-52643d8d85e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def blur_images(image):\n",
        "  return cv2.GaussianBlur(image,(5,5),0)"
      ],
      "metadata": {
        "id": "SeNNEVyIKJPE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "lfw_dataset = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
        "images = lfw_dataset.images\n",
        "labels = lfw_dataset.target\n",
        "label_names = lfw_dataset.target_names"
      ],
      "metadata": {
        "id": "ALWYJDHjKJYm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blurred_images = np.array([blur_images(img) for img in images])\n",
        "augumented_images = np.concatenate((images,blurred_images))\n",
        "augumented_lables = np.concatenate((labels,labels))"
      ],
      "metadata": {
        "id": "FA93wq-OKJe-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augumented_images = augumented_images / 255.0\n",
        "augumented_images = augumented_images.reshape(augumented_images.shape[0],augumented_images.shape[1],augumented_images.shape[2],1)\n"
      ],
      "metadata": {
        "id": "xZCB0aDVKJjk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(augumented_images,augumented_lables,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "WFChROoEKJn8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    layers.Conv2D(32,(3,3),activation='relu',input_shape=(images.shape[1],images.shape[2],1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64,(3,3),activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(128,(3,3),activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128,activation='relu'),\n",
        "    layers.Dense(len(label_names),activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJgMaXeTKJsB",
        "outputId": "68fb8788-d3f1-4d12-f376-61d210ff114b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)\n",
        "\n",
        "# Save the model\n",
        "model.save('fine_tuned_facial_recognition_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZJmHVqfZKJwF",
        "outputId": "91f08021-b63a-41f6-f551-53b0fa6a78a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.4260 - loss: 1.7773 - val_accuracy: 0.3973 - val_loss: 1.6983\n",
            "Epoch 2/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4204 - loss: 1.6886 - val_accuracy: 0.3973 - val_loss: 1.7161\n",
            "Epoch 3/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4192 - loss: 1.6761 - val_accuracy: 0.3973 - val_loss: 1.7153\n",
            "Epoch 4/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3995 - loss: 1.7098 - val_accuracy: 0.3973 - val_loss: 1.7459\n",
            "Epoch 5/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4222 - loss: 1.6757 - val_accuracy: 0.3973 - val_loss: 1.6990\n",
            "Epoch 6/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4157 - loss: 1.6879 - val_accuracy: 0.3973 - val_loss: 1.7023\n",
            "Epoch 7/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4321 - loss: 1.6513 - val_accuracy: 0.3973 - val_loss: 1.7045\n",
            "Epoch 8/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4269 - loss: 1.6746 - val_accuracy: 0.3973 - val_loss: 1.7000\n",
            "Epoch 9/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4116 - loss: 1.6943 - val_accuracy: 0.3973 - val_loss: 1.7003\n",
            "Epoch 10/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4133 - loss: 1.6733 - val_accuracy: 0.3973 - val_loss: 1.7006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Augment training data\n",
        "datagen.fit(X_train)\n"
      ],
      "metadata": {
        "id": "2XP03atZKJ0V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iuxanj1KJ8Y",
        "outputId": "295ecd36-4ecf-412b-9fd5-12bb11c9eaf8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2060, 50, 37, 1) (516, 50, 37, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
        "\n",
        "# Load pre-trained MobileNetV2\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(50, 50, 3))\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False  # Freeze pre-trained layers\n",
        "\n",
        "# Add custom layers for fine-tuning\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(len(set(y_train)), activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtAAw5OjKKAb",
        "outputId": "8374b115-e6c8-4380-a91d-fb0f0d7e4acf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-3cad33e4a5a7>:6: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(50, 50, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.image import resize\n",
        "X_train = np.array([resize(img, (50, 50)).numpy() for img in X_train])\n",
        "X_test = np.array([resize(img, (50, 50)).numpy() for img in X_test])\n"
      ],
      "metadata": {
        "id": "eadwGlW5KKEN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n"
      ],
      "metadata": {
        "id": "HvD4DKwaKKIJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1mNi32DKKL6",
        "outputId": "1816480a-2dcc-475c-8e21-5da898f3c975"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2060, 50, 50, 1) (516, 50, 50, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert grayscale (50, 50, 1) to RGB (50, 50, 3)\n",
        "X_train = np.repeat(X_train, 3, axis=-1)  # Repeat the single channel 3 times\n",
        "X_test = np.repeat(X_test, 3, axis=-1)\n",
        "\n",
        "print(X_train.shape, X_test.shape)  # Should now be (2060, 50, 50, 3) and (516, 50, 50, 3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dj8bIJWKKPh",
        "outputId": "ba6870c3-b9b9-4e45-d180-43662925f7e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2060, 50, 50, 3) (516, 50, 50, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n"
      ],
      "metadata": {
        "id": "7bWmzZa-KKTN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=20,\n",
        "    batch_size=32\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uVzoQAEKKXA",
        "outputId": "10a87c4a-41ca-4358-fb71-9c6441c0f1d9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 152ms/step - accuracy: 0.2803 - loss: 2.8284 - val_accuracy: 0.3973 - val_loss: 1.7143\n",
            "Epoch 2/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.3984 - loss: 1.7656 - val_accuracy: 0.3973 - val_loss: 1.7281\n",
            "Epoch 3/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4007 - loss: 1.7619 - val_accuracy: 0.3973 - val_loss: 1.7193\n",
            "Epoch 4/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4216 - loss: 1.7298 - val_accuracy: 0.3973 - val_loss: 1.7016\n",
            "Epoch 5/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4198 - loss: 1.7170 - val_accuracy: 0.3973 - val_loss: 1.7233\n",
            "Epoch 6/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4179 - loss: 1.7311 - val_accuracy: 0.3973 - val_loss: 1.7007\n",
            "Epoch 7/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4106 - loss: 1.7249 - val_accuracy: 0.3973 - val_loss: 1.7053\n",
            "Epoch 8/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4249 - loss: 1.7189 - val_accuracy: 0.3973 - val_loss: 1.7037\n",
            "Epoch 9/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4136 - loss: 1.7219 - val_accuracy: 0.3973 - val_loss: 1.7098\n",
            "Epoch 10/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4271 - loss: 1.7081 - val_accuracy: 0.3973 - val_loss: 1.6993\n",
            "Epoch 11/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4252 - loss: 1.7043 - val_accuracy: 0.3973 - val_loss: 1.7094\n",
            "Epoch 12/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4128 - loss: 1.7185 - val_accuracy: 0.3973 - val_loss: 1.7106\n",
            "Epoch 13/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4220 - loss: 1.7000 - val_accuracy: 0.3973 - val_loss: 1.6994\n",
            "Epoch 14/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4426 - loss: 1.6639 - val_accuracy: 0.3973 - val_loss: 1.7021\n",
            "Epoch 15/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4049 - loss: 1.7143 - val_accuracy: 0.3973 - val_loss: 1.6989\n",
            "Epoch 16/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4175 - loss: 1.7012 - val_accuracy: 0.3973 - val_loss: 1.7033\n",
            "Epoch 17/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4164 - loss: 1.6946 - val_accuracy: 0.3973 - val_loss: 1.7029\n",
            "Epoch 18/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4131 - loss: 1.7005 - val_accuracy: 0.3973 - val_loss: 1.7002\n",
            "Epoch 19/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4109 - loss: 1.7118 - val_accuracy: 0.3973 - val_loss: 1.7071\n",
            "Epoch 20/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4067 - loss: 1.6992 - val_accuracy: 0.3973 - val_loss: 1.6974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=32),\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=20\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nacy2V03KKa4",
        "outputId": "04213718-4e9a-4752-87df-53e30616cde7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.4172 - loss: 1.6873 - val_accuracy: 0.3973 - val_loss: 1.7024\n",
            "Epoch 2/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.4057 - loss: 1.6965 - val_accuracy: 0.3973 - val_loss: 1.7024\n",
            "Epoch 3/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.4291 - loss: 1.6714 - val_accuracy: 0.3973 - val_loss: 1.7038\n",
            "Epoch 4/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.3990 - loss: 1.7126 - val_accuracy: 0.3973 - val_loss: 1.7057\n",
            "Epoch 5/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.4055 - loss: 1.6898 - val_accuracy: 0.3973 - val_loss: 1.7038\n",
            "Epoch 6/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.4114 - loss: 1.6942 - val_accuracy: 0.3973 - val_loss: 1.7013\n",
            "Epoch 7/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.4147 - loss: 1.6901 - val_accuracy: 0.3973 - val_loss: 1.7012\n",
            "Epoch 8/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.4098 - loss: 1.7069 - val_accuracy: 0.3973 - val_loss: 1.7009\n",
            "Epoch 9/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.4074 - loss: 1.7017 - val_accuracy: 0.3973 - val_loss: 1.6972\n",
            "Epoch 10/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.3974 - loss: 1.7122 - val_accuracy: 0.3973 - val_loss: 1.6996\n",
            "Epoch 11/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4090 - loss: 1.6891 - val_accuracy: 0.3973 - val_loss: 1.6997\n",
            "Epoch 12/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4116 - loss: 1.6863 - val_accuracy: 0.3973 - val_loss: 1.7006\n",
            "Epoch 13/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4240 - loss: 1.6786 - val_accuracy: 0.3973 - val_loss: 1.6987\n",
            "Epoch 14/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - accuracy: 0.4148 - loss: 1.6817 - val_accuracy: 0.3973 - val_loss: 1.7009\n",
            "Epoch 15/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.4127 - loss: 1.7048 - val_accuracy: 0.3973 - val_loss: 1.7016\n",
            "Epoch 16/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.4144 - loss: 1.6911 - val_accuracy: 0.3973 - val_loss: 1.6992\n",
            "Epoch 17/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.4065 - loss: 1.6999 - val_accuracy: 0.3973 - val_loss: 1.7016\n",
            "Epoch 18/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.4268 - loss: 1.6722 - val_accuracy: 0.3973 - val_loss: 1.7013\n",
            "Epoch 19/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.3980 - loss: 1.7021 - val_accuracy: 0.3973 - val_loss: 1.7016\n",
            "Epoch 20/20\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.4214 - loss: 1.6751 - val_accuracy: 0.3973 - val_loss: 1.7017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('fine_tuned_facial_recognition_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SPZbQRKKKel",
        "outputId": "519b2ae6-b940-4e3e-d702-699e08127a74"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VuLWRw-xKKid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image from file (replace with your test image path)\n",
        "def load_image_from_file(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
        "    image = cv2.resize(image, (50, 50))  # Resize image\n",
        "    image = np.repeat(image[..., np.newaxis], 3, axis=-1)  # Convert to 3 channels\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image.reshape(1, 50, 50, 3)  # Reshape for model input\n",
        "\n",
        "# Test with a sample ID image and webcam image\n",
        "id_image_path = '/content/Nikil passport_1.jpg'  # Replace with your ID image path\n",
        "id_image = load_image_from_file(id_image_path)\n",
        "\n",
        "# Replace webcam image with another sample image for testing\n",
        "test_image_path = '/content/DSC_0986 copy.jpg'  # Replace with another test image\n",
        "webcam_image = load_image_from_file(test_image_path)\n",
        "\n",
        "match_percentage = match_faces(id_image, webcam_image)\n",
        "if match_percentage > 85:\n",
        "    print(f\"Match Successful! Match Percentage: {match_percentage:.2f}%\")\n",
        "else:\n",
        "    print(f\"Match Failed. Match Percentage: {match_percentage:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "collapsed": true,
        "id": "YhnrLZ1RRAew",
        "outputId": "ae20d0cd-8175-4f4d-fa60-86de7dfdb889"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 7500 into shape (1,50,50,1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5e28d5759944>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mwebcam_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmatch_percentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwebcam_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmatch_percentage\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m85\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Match Successful! Match Percentage: {match_percentage:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-7a28b837c7c1>\u001b[0m in \u001b[0;36mmatch_faces\u001b[0;34m(id_image, webcam_image)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmatch_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwebcam_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mid_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_image\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mid_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mwebcam_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebcam_image\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mwebcam_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebcam_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 7500 into shape (1,50,50,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load pre-trained model\n",
        "model = load_model('fine_tuned_facial_recognition_model.h5')\n",
        "\n",
        "# Convert grayscale image (50, 50, 1) to RGB (50, 50, 3)\n",
        "def load_image_from_file(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
        "    image = cv2.resize(image, (50, 50))  # Resize image\n",
        "    image = np.repeat(image[..., np.newaxis], 3, axis=-1)  # Convert to 3 channels\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image.reshape(1, 50, 50, 3)  # Reshape for model input\n",
        "\n",
        "# Load ID and test images\n",
        "id_image_path = '/content/Nikil passport_1.jpg'  # Replace with your ID image path\n",
        "id_image = load_image_from_file(id_image_path)\n",
        "\n",
        "# Replace with the actual path for a test face image\n",
        "test_image_path = '/content/DSC_0986 copy.jpg'  # Replace with your test image path\n",
        "webcam_image = load_image_from_file(test_image_path)\n",
        "\n",
        "# Perform face matching\n",
        "def match_faces(id_image, webcam_image):\n",
        "    id_image = id_image / 255.0  # Normalize to [0, 1]\n",
        "    webcam_image = webcam_image / 255.0\n",
        "\n",
        "    # Predict embeddings (features) for both ID and webcam images\n",
        "    id_embedding = model.predict(id_image)\n",
        "    webcam_embedding = model.predict(webcam_image)\n",
        "\n",
        "    # Calculate cosine similarity between the embeddings\n",
        "    similarity = np.dot(id_embedding, webcam_embedding.T) / (\n",
        "        np.linalg.norm(id_embedding) * np.linalg.norm(webcam_embedding)\n",
        "    )\n",
        "\n",
        "    return similarity.item() * 100  # Percentage match\n",
        "\n",
        "# Calculate match percentage\n",
        "match_percentage = match_faces(id_image, webcam_image)\n",
        "if match_percentage > 85:\n",
        "    print(f\"Match Successful! Match Percentage: {match_percentage:.2f}%\")\n",
        "else:\n",
        "    print(f\"Match Failed. Match Percentage: {match_percentage:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zngdB0JwRAiz",
        "outputId": "8acbe55d-cc11-419c-811a-ca79f9690c7d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Match Successful! Match Percentage: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "facial-recognition-bucket-nk"
      ],
      "metadata": {
        "id": "OHX6ssqQRAmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "NOZD7E8VRApb",
        "outputId": "f9c05c43-27e2-4452-f704-8f7ac6ef14be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 1) (<ipython-input-1-d4085be98b5b>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d4085be98b5b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(tf.__version__)\"\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wKr6RUvfRAst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11KYpVq9RAwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gH0zyJNHRAza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-9VfgcCARA2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnWWSkV4RA6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "id": "eh09pKRZRA9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LA73G3ANRBEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nkxEWjIcRBHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wlRBrCw7RBWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kNhlswo3RBZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qEXoxMv-RBdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J5OmomsoRBgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N91BzIVARBj4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}